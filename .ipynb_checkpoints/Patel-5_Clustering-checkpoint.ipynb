{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "higher-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "'''Main'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time\n",
    "import pickle, gzip\n",
    "\n",
    "'''Data Viz'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "'''Data Prep and Model Evaluation'''\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# Downloading the data\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Dimension Reductions\n",
    "# Principal Component Analysis\n",
    "from sklearn.decomposition import PCA\n",
    "# Incremental PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "# Sparse PCA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "# Kernel PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# Singular Value Decomposition\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Gaussian Random Projection\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "# Sparse Random Projection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "# Isomap\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "# Multidimensional Scaling\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "# Locally Linear Embedding (LLE)\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "# t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Mini-batch dictionary learning\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
    "\n",
    "# Independent Component Analysis\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "import sys\n",
    "# !{sys.executable} -m pip install fastcluster\n",
    "# import fastcluster\n",
    "\n",
    "import fastcluster\n",
    "from scipy.cluster.hierarchy import dendrogram, cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.cluster import DBSCAN\n",
    "# import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "removable-ireland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hdbscan\n",
      "  Downloading hdbscan-0.8.27.tar.gz (6.4 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting joblib>=1.0\n",
      "  Using cached joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\mtwpi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from hdbscan) (1.21.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in c:\\users\\mtwpi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from hdbscan) (0.24.1)\n",
      "Collecting cython>=0.27\n",
      "  Using cached Cython-0.29.24-cp38-cp38-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied: six in c:\\users\\mtwpi\\appdata\\roaming\\python\\python38\\site-packages (from hdbscan) (1.15.0)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\mtwpi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from hdbscan) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mtwpi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn>=0.20->hdbscan) (2.1.0)\n",
      "Building wheels for collected packages: hdbscan\n",
      "  Building wheel for hdbscan (PEP 517): started\n",
      "  Building wheel for hdbscan (PEP 517): finished with status 'error'\n",
      "Failed to build hdbscan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\python.exe' 'C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py' build_wheel 'C:\\Users\\mtwpi\\AppData\\Local\\Temp\\tmp70yad95c'\n",
      "       cwd: C:\\Users\\mtwpi\\AppData\\Local\\Temp\\pip-install-ntjgxedf\\hdbscan_d436b5d4c6424733a79071af86bec4a1\n",
      "  Complete output (27 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.8\n",
      "  creating build\\lib.win-amd64-3.8\\hdbscan\n",
      "  copying hdbscan\\flat.py -> build\\lib.win-amd64-3.8\\hdbscan\n",
      "  copying hdbscan\\hdbscan_.py -> build\\lib.win-amd64-3.8\\hdbscan\n",
      "  copying hdbscan\\plots.py -> build\\lib.win-amd64-3.8\\hdbscan\n",
      "  copying hdbscan\\prediction.py -> build\\lib.win-amd64-3.8\\hdbscan\n",
      "  copying hdbscan\\robust_single_linkage_.py -> build\\lib.win-amd64-3.8\\hdbscan\n",
      "  copying hdbscan\\validity.py -> build\\lib.win-amd64-3.8\\hdbscan\n",
      "  copying hdbscan\\__init__.py -> build\\lib.win-amd64-3.8\\hdbscan\n",
      "  creating build\\lib.win-amd64-3.8\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_flat.py -> build\\lib.win-amd64-3.8\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_hdbscan.py -> build\\lib.win-amd64-3.8\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_rsl.py -> build\\lib.win-amd64-3.8\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\__init__.py -> build\\lib.win-amd64-3.8\\hdbscan\\tests\n",
      "  running build_ext\n",
      "  skipping 'hdbscan\\_hdbscan_tree.c' Cython extension (up-to-date)\n",
      "  skipping 'hdbscan\\_hdbscan_linkage.c' Cython extension (up-to-date)\n",
      "  skipping 'hdbscan\\_hdbscan_boruvka.c' Cython extension (up-to-date)\n",
      "  skipping 'hdbscan\\_hdbscan_reachability.c' Cython extension (up-to-date)\n",
      "  skipping 'hdbscan\\_prediction_utils.c' Cython extension (up-to-date)\n",
      "  skipping 'hdbscan\\dist_metrics.c' Cython extension (up-to-date)\n",
      "  building 'hdbscan._hdbscan_tree' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for hdbscan\n",
      "ERROR: Could not build wheels for hdbscan which use PEP 517 and cannot be installed directly\n",
      "WARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hdbscan'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3585462ee4f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# To upgrade numpy?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# !{sys.executable} pip install --upgrade numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mhdbscan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'hdbscan'"
     ]
    }
   ],
   "source": [
    "# !{sys.executable} -m pip install hdbscan\n",
    "# To upgrade numpy?\n",
    "# !{sys.executable} pip install --upgrade numpy\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "regulated-navigator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(X_train.shape)\n",
    "\n",
    "\n",
    "# Reshaping data\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1]**2))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1]**2))\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "y_train = pd.Series(y_train)\n",
    "y_test = pd.Series(y_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "\n",
    "# Put training data together\n",
    "bigDF1 = pd.concat([X_train, y_train], axis = 1)\n",
    "\n",
    "# Getting train and valid data\n",
    "train, valid = train_test_split(bigDF1, test_size=0.2, random_state = 420, shuffle=True)\n",
    "\n",
    "y_train = train.iloc[:, -1]\n",
    "y_valid = valid.iloc[:, -1]\n",
    "\n",
    "X_train = train.iloc[:, 0:-1]\n",
    "X_valid = train.iloc[:, 0:-1]\n",
    "\n",
    "# pd.concat([X_train, X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "blocked-telling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pandas DataFrames from the datasets\n",
    "train_index = range(0,len(X_train))\n",
    "validation_index = range(len(X_train), \\\n",
    " len(X_train)+len(X_valid))\n",
    "test_index = range(len(X_train)+len(X_valid), \\\n",
    "len(X_train)+len(X_valid)+len(X_test))\n",
    "\n",
    "# pd.concat([X_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "forbidden-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 784\n",
    "whiten = False\n",
    "random_state = 2018\n",
    "pca = PCA(n_components=n_components, whiten=whiten, \\\n",
    "random_state=random_state)\n",
    "X_train_PCA = pca.fit_transform(X_train)\n",
    "X_train_PCA = pd.DataFrame(data=X_train_PCA, index=train_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-definition",
   "metadata": {},
   "source": [
    "We did not reduce dimensionality, but we'll choose the number of principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "manufactured-cookbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n"
     ]
    }
   ],
   "source": [
    "# k-means - Inertia as the number of clusters varies\n",
    "from sklearn.cluster import KMeans\n",
    "n_clusters = 10\n",
    "n_init = 10\n",
    "max_iter = 300\n",
    "tol = 0.0001\n",
    "random_state = 420\n",
    "n_jobs = 2\n",
    "kMeans_inertia = pd.DataFrame(data=[],index=range(2,21), \\\n",
    "    columns=['inertia'])\n",
    "\n",
    "for n_clusters in range(2,21):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=n_init, \\\n",
    "        max_iter=max_iter, tol=tol, random_state=random_state, \\\n",
    "        n_jobs=n_jobs)\n",
    "    cutoff = 99\n",
    "    kmeans.fit(X_train_PCA.loc[:,0:cutoff])\n",
    "    kMeans_inertia.loc[n_clusters] = kmeans.inertia_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "scientific-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeCluster(clusterDF, labelsDF):\n",
    "    countByCluster = \\\n",
    "        pd.DataFrame(data=clusterDF['cluster'].value_counts())\n",
    "    \n",
    "    # # rows by cluster\n",
    "    countByCluster.reset_index(inplace=True,drop=False)\n",
    "    countByCluster.columns = ['cluster','clusterCount']\n",
    "    \n",
    "    preds = pd.concat([labelsDF,clusterDF], axis=1)\n",
    "    preds.columns = ['trueLabel','cluster']\n",
    "\n",
    "\n",
    "    # observations by true label from train data\n",
    "    countByLabel = pd.DataFrame(data=preds.groupby('trueLabel').count())\n",
    "    \n",
    "    countMostFreq = \\\n",
    "        pd.DataFrame(data=preds.groupby('cluster').agg( \\\n",
    "        lambda x:x.value_counts().iloc[0]))\n",
    "    countMostFreq.reset_index(inplace=True,drop=False)\n",
    "    countMostFreq.columns = ['cluster','countMostFrequent']\n",
    "    \n",
    "    accuracyDF = countMostFreq.merge(countByCluster, \\\n",
    "    left_on=\"cluster\",right_on=\"cluster\")\n",
    "    overallAccuracy = accuracyDF.countMostFrequent.sum()/ \\\n",
    "    accuracyDF.clusterCount.sum()\n",
    "    accuracyByLabel = accuracyDF.countMostFrequent/ \\\n",
    "        accuracyDF.clusterCount\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fantastic-digest",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6b297efdb9a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         pd.DataFrame(data=X_train_kmeansClustered, index=X_train.index, \\\n\u001b[0;32m     23\u001b[0m         columns=['cluster'])\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mcountByCluster_kMeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcountByLabel_kMeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcountMostFreq_kMeans\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0maccuracyDF_kMeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverallAccuracy_kMeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracyByLabel_kMeans\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;33m=\u001b[0m \u001b[0manalyzeCluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_kmeansClustered\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "n_clusters = 5\n",
    "n_init = 10\n",
    "max_iter = 300\n",
    "tol = 0.0001\n",
    "random_state = 2018\n",
    "n_jobs = 2\n",
    "kMeans_inertia = \\\n",
    "    pd.DataFrame(data=[],index=range(2,21),columns=['inertia'])\n",
    "overallAccuracy_kMeansDF = \\\n",
    "    pd.DataFrame(data=[],index=range(2,21),columns=['overallAccuracy'])\n",
    "\n",
    "for n_clusters in range(2,21):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=n_init, \\\n",
    "        max_iter=max_iter, tol=tol, random_state=random_state, \\\n",
    "        n_jobs=n_jobs)\n",
    "    cutoff = 99\n",
    "    kmeans.fit(X_train_PCA.loc[:,0:cutoff])\n",
    "    kMeans_inertia.loc[n_clusters] = kmeans.inertia_\n",
    "    X_train_kmeansClustered = kmeans.predict(X_train_PCA.loc[:,0:cutoff])\n",
    "    \n",
    "    X_train_kmeansClustered = \\\n",
    "        pd.DataFrame(data=X_train_kmeansClustered, index=X_train.index, \\\n",
    "        columns=['cluster'])\n",
    "    countByCluster_kMeans, countByLabel_kMeans, countMostFreq_kMeans, \\\n",
    "        accuracyDF_kMeans, overallAccuracy_kMeans, accuracyByLabel_kMeans \\\n",
    "        = analyzeCluster(X_train_kmeansClustered, y_train)\n",
    "    overallAccuracy_kMeansDF.loc[n_clusters] = overallAccuracy_kMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "first-castle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mtwpi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "<ipython-input-10-3e618d752f7a>:20: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (20). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(X_train.loc[:,0:cutoff])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-3e618d752f7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     X_train_kmeansClustered = pd.DataFrame(data=X_train_kmeansClustered, \\\n\u001b[0;32m     24\u001b[0m         index=X_train.index, columns=['cluster'])\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mcountByCluster_kMeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcountByLabel_kMeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcountMostFreq_kMeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracyDF_kMeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverallAccuracy_kMeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracyByLabel_kMeans\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;33m=\u001b[0m \u001b[0manalyzeCluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_kmeansClustered\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0moverallAccuracy_kMeansDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moverallAccuracy_kMeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# k-means - Accuracy as the number of components varies\n",
    "# On the original MNIST data (not PCA-reduced)\n",
    "n_clusters = 20\n",
    "n_init = 10\n",
    "max_iter = 300\n",
    "tol = 0.0001\n",
    "random_state = 2018\n",
    "n_jobs = 2\n",
    "kMeans_inertia = pd.DataFrame(data=[],index=[9, 49, 99, 199, \\\n",
    "    299, 399, 499, 599, 699, 784],columns=['inertia'])\n",
    "overallAccuracy_kMeansDF = pd.DataFrame(data=[],index=[9, 49, \\\n",
    "    99, 199, 299, 399, 499, 599, 699, 784], \\\n",
    "    columns=['overallAccuracy'])\n",
    "\n",
    "for cutoffNumber in [9, 49, 99, 199, 299, 399, 499, 599, 699, 784]:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=n_init, \\\n",
    "        max_iter=max_iter, tol=tol, random_state=random_state, \\\n",
    "        n_jobs=n_jobs)\n",
    "    cutoff = cutoffNumber\n",
    "    kmeans.fit(X_train.loc[:,0:cutoff])\n",
    "    kMeans_inertia.loc[cutoff] = kmeans.inertia_\n",
    "    X_train_kmeansClustered = kmeans.predict(X_train.loc[:,0:cutoff])\n",
    "    X_train_kmeansClustered = pd.DataFrame(data=X_train_kmeansClustered, \\\n",
    "        index=X_train.index, columns=['cluster'])\n",
    "    countByCluster_kMeans, countByLabel_kMeans, countMostFreq_kMeans, accuracyDF_kMeans, overallAccuracy_kMeans, accuracyByLabel_kMeans \\\n",
    "        = analyzeCluster(X_train_kmeansClustered, y_train)\n",
    "    overallAccuracy_kMeansDF.loc[cutoff] = overallAccuracy_kMeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-amount",
   "metadata": {},
   "source": [
    "## Spawning A Dendogram (Upside-Down Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-vitamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 100\n",
    "Z = fastcluster.linkage_vector(X_train_PCA.loc[:,0:cutoff], \\\n",
    "    method='ward', metric='euclidean')\n",
    "Z_dataFrame = pd.DataFrame(data=Z, \\\n",
    "    columns=['clusterOne','clusterTwo','distance','newClusterSize'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-lindsay",
   "metadata": {},
   "source": [
    "## Hierarchal Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "greater-collapse",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-dd3b8511c33a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdistance_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m160\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfcluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistance_threshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'distance'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_train_hierClustered\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclusters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train_PCA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cluster'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Z' is not defined"
     ]
    }
   ],
   "source": [
    "distance_threshold = 160\n",
    "clusters = fcluster(Z, distance_threshold, criterion='distance')\n",
    "X_train_hierClustered = \\\n",
    "    pd.DataFrame(data=clusters,index=X_train_PCA.index,columns=['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 3\n",
    "min_samples = 5\n",
    "leaf_size = 30\n",
    "n_jobs = 4\n",
    "db = DBSCAN(eps=eps, min_samples=min_samples, leaf_size=leaf_size,\n",
    "    n_jobs=n_jobs)\n",
    "\n",
    "cutoff = 99\n",
    "X_train_PCA_dbscanClustered = db.fit_predict(X_train_PCA.loc[:,0:cutoff])\n",
    "X_train_PCA_dbscanClustered = \\\n",
    "    pd.DataFrame(data=X_train_PCA_dbscanClustered, index=X_train.index, \\\n",
    "    columns=['cluster'])\n",
    "\n",
    "countByCluster_dbscan, countByLabel_dbscan, countMostFreq_dbscan, \\\n",
    "    accuracyDF_dbscan, overallAccuracy_dbscan, accuracyByLabel_dbscan \\\n",
    "    = analyzeCluster(X_train_PCA_dbscanClustered, y_train)\n",
    "overallAccuracy_dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_size = 30\n",
    "min_samples = None\n",
    "alpha = 1.0\n",
    "cluster_selection_method = 'eom'\n",
    "hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, \\\n",
    "    min_samples=min_samples, alpha=alpha, \\\n",
    "    cluster_selection_method=cluster_selection_method)\n",
    "\n",
    "cutoff = 10\n",
    "X_train_PCA_hdbscanClustered = \\\n",
    "    hdb.fit_predict(X_train_PCA.loc[:,0:cutoff])\n",
    "\n",
    "X_train_PCA_hdbscanClustered = \\\n",
    "    pd.DataFrame(data=X_train_PCA_hdbscanClustered, \\\n",
    "    index=X_train.index, columns=['cluster'])\n",
    "\n",
    "countByCluster_hdbscan, countByLabel_hdbscan, \\\n",
    "    countMostFreq_hdbscan, accuracyDF_hdbscan, \\\n",
    "    overallAccuracy_hdbscan, accuracyByLabel_hdbscan \\\n",
    "    = analyzeCluster(X_train_PCA_hdbscanClustered, y_train)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
